{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ae1458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import easyocr\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd771c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Processing image:  Sarthi Studios-1069.jpg\n",
      "\n",
      "image 1/1 /Users/phoenixa/Documents/projects/marathon/Edited/Sarthi Studios-1069.jpg: 640x480 1 person, 65.1ms\n",
      "Speed: 5.5ms preprocess, 65.1ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/marathon/lib/python3.13/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Processing image:  Sarthi Studios-879.jpg\n",
      "\n",
      "image 1/1 /Users/phoenixa/Documents/projects/marathon/Edited/Sarthi Studios-879.jpg: 640x512 1 person, 1 cell phone, 63.6ms\n",
      "Speed: 4.8ms preprocess, 63.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 512)\n",
      "0\n",
      "1\n",
      "2\n",
      "Processing image:  Sarthi Studios-1055.jpg\n",
      "\n",
      "image 1/1 /Users/phoenixa/Documents/projects/marathon/Edited/Sarthi Studios-1055.jpg: 640x512 13 persons, 146.0ms\n",
      "Speed: 21.3ms preprocess, 146.0ms inference, 42.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "3\n",
      "Processing image:  Sarthi Studios-851.jpg\n",
      "\n",
      "image 1/1 /Users/phoenixa/Documents/projects/marathon/Edited/Sarthi Studios-851.jpg: 640x512 8 persons, 155.3ms\n",
      "Speed: 16.6ms preprocess, 155.3ms inference, 10.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "4\n",
      "Processing image:  Sarthi Studios-689.jpg\n",
      "\n",
      "image 1/1 /Users/phoenixa/Documents/projects/marathon/Edited/Sarthi Studios-689.jpg: 640x480 4 persons, 425.2ms\n",
      "Speed: 99.4ms preprocess, 425.2ms inference, 23.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "Processing image:  Sarthi Studios-1733.jpg\n",
      "\n",
      "image 1/1 /Users/phoenixa/Documents/projects/marathon/Edited/Sarthi Studios-1733.jpg: 640x512 6 books, 201.0ms\n",
      "Speed: 10.3ms preprocess, 201.0ms inference, 14.7ms postprocess per image at shape (1, 3, 640, 512)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Processing image:  Sarthi Studios-1727.jpg\n",
      "\n",
      "image 1/1 /Users/phoenixa/Documents/projects/marathon/Edited/Sarthi Studios-1727.jpg: 640x480 1 person, 135.1ms\n",
      "Speed: 8.5ms preprocess, 135.1ms inference, 10.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "0\n",
      "7\n",
      "Processing image:  Sarthi Studios-1041.jpg\n",
      "\n",
      "image 1/1 /Users/phoenixa/Documents/projects/marathon/Edited/Sarthi Studios-1041.jpg: 640x512 18 persons, 1 bicycle, 1 umbrella, 91.0ms\n",
      "Speed: 4.7ms preprocess, 91.0ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 512)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "8\n",
      "Processing image:  Sarthi Studios-845.jpg\n",
      "\n",
      "image 1/1 /Users/phoenixa/Documents/projects/marathon/Edited/Sarthi Studios-845.jpg: 640x512 10 persons, 1 truck, 92.9ms\n",
      "Speed: 12.5ms preprocess, 92.9ms inference, 18.3ms postprocess per image at shape (1, 3, 640, 512)\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m crop = img[y1:y2, x1:x2]\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Run OCR on each detected region\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m ocr_result = \u001b[43mocr_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, text, conf \u001b[38;5;129;01min\u001b[39;00m ocr_result:\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Only keep numeric-looking results (e.g. \"1024\", \"57A\")\u001b[39;00m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text.strip()) >= \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(ch.isdigit() \u001b[38;5;28;01mfor\u001b[39;00m ch \u001b[38;5;129;01min\u001b[39;00m text):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/marathon/lib/python3.13/site-packages/easyocr/easyocr.py:456\u001b[39m, in \u001b[36mReader.readtext\u001b[39m\u001b[34m(self, image, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, min_size, contrast_ths, adjust_contrast, filter_ths, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, y_ths, x_ths, add_margin, threshold, bbox_min_score, bbox_min_size, max_candidates, output_format)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[33;03mParameters:\u001b[39;00m\n\u001b[32m    452\u001b[39m \u001b[33;03mimage: file path or numpy-array or a byte stream object\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m    454\u001b[39m img, img_cv_grey = reformat_input(image)\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m horizontal_list, free_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mmin_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mslope_ths\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mslope_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mycenter_ths\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mycenter_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mheight_ths\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth_ths\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth_ths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43madd_margin\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_margin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreformat\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_candidates\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# get the 1st result from hor & free list as self.detect returns a list of depth 3\u001b[39;00m\n\u001b[32m    467\u001b[39m horizontal_list, free_list = horizontal_list[\u001b[32m0\u001b[39m], free_list[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/marathon/lib/python3.13/site-packages/easyocr/easyocr.py:321\u001b[39m, in \u001b[36mReader.detect\u001b[39m\u001b[34m(self, img, min_size, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, add_margin, reformat, optimal_num_chars, threshold, bbox_min_score, bbox_min_size, max_candidates)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reformat:\n\u001b[32m    319\u001b[39m     img, img_cv_grey = reformat_input(img)\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m text_box_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_textbox\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    326\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m                            \u001b[49m\u001b[43moptimal_num_chars\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimal_num_chars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_min_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m horizontal_list_agg, free_list_agg = [], []\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text_box \u001b[38;5;129;01min\u001b[39;00m text_box_list:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/marathon/lib/python3.13/site-packages/easyocr/detection.py:95\u001b[39m, in \u001b[36mget_textbox\u001b[39m\u001b[34m(detector, image, canvas_size, mag_ratio, text_threshold, link_threshold, low_text, poly, device, optimal_num_chars, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m result = []\n\u001b[32m     94\u001b[39m estimate_num_chars = optimal_num_chars \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m bboxes_list, polys_list = \u001b[43mtest_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcanvas_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmag_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mlink_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate_num_chars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimate_num_chars:\n\u001b[32m    100\u001b[39m     polys_list = [[p \u001b[38;5;28;01mfor\u001b[39;00m p, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(polys, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mabs\u001b[39m(optimal_num_chars - x[\u001b[32m1\u001b[39m]))]\n\u001b[32m    101\u001b[39m                   \u001b[38;5;28;01mfor\u001b[39;00m polys \u001b[38;5;129;01min\u001b[39;00m polys_list]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/marathon/lib/python3.13/site-packages/easyocr/detection.py:51\u001b[39m, in \u001b[36mtest_net\u001b[39m\u001b[34m(canvas_size, mag_ratio, net, image, text_threshold, link_threshold, low_text, poly, device, estimate_num_chars)\u001b[39m\n\u001b[32m     48\u001b[39m boxes_list, polys_list = [], []\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m y:\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# make score and link map\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     score_text = \u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.data.numpy()\n\u001b[32m     52\u001b[39m     score_link = out[:, :, \u001b[32m1\u001b[39m].cpu().data.numpy()\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# Post-processing\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Input/output paths\n",
    "input_folder = Path(\"/Users/phoenixa/Documents/projects/marathon/Edited\")\n",
    "output_folder = Path(\"/Users/phoenixa/Documents/projects/marathon/output\")\n",
    "output_folder.mkdir(exist_ok=True)\n",
    "csv_file = output_folder / \"bib_numbers.csv\"\n",
    "\n",
    "# Load YOLO model (use your fine-tuned bib detector if available)\n",
    "yolo_model = YOLO(\"yolov8n.pt\")\n",
    "ocr_reader = easyocr.Reader(['en'])\n",
    "\n",
    "results_data = []\n",
    "count=0\n",
    "for img_path in input_folder.glob(\"*.jpg\"):\n",
    "    print(count)\n",
    "    count+=1\n",
    "    print(\"Processing image: \", img_path.name)\n",
    "    img = cv2.imread(str(img_path))\n",
    "    detections = yolo_model(img_path)\n",
    "\n",
    "    # Loop over all detected boxes\n",
    "    box_count=0\n",
    "    for box in detections[0].boxes: \n",
    "        print('box_count:',box_count)\n",
    "        box_count+=1\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        crop = img[y1:y2, x1:x2]\n",
    "\n",
    "        # Run OCR on each detected region\n",
    "        ocr_result = ocr_reader.readtext(crop)\n",
    "        for _, text, conf in ocr_result:\n",
    "            # Only keep numeric-looking results (e.g. \"1024\", \"57A\")\n",
    "            if len(text.strip()) >= 2 and any(ch.isdigit() for ch in text):\n",
    "                results_data.append({\n",
    "                    \"image\": img_path.name,\n",
    "                    \"bib_number\": text.strip(),\n",
    "                    \"confidence\": conf,\n",
    "                    \"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2\n",
    "                })\n",
    "\n",
    "# Save all detections to CSV\n",
    "pd.DataFrame(results_data).to_csv(csv_file, index=False)\n",
    "print(f\"âœ… Saved {len(results_data)} bibs to {csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
